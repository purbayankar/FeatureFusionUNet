{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Proposed FeatureFusionUNet.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purbayankar/FeatureFusionUNet/blob/main/Proposed_FeatureFusionUNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxcLflB9uMT8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51f0999f-fa28-4211-83f6-0773a3138b64"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBK-VDWitCCD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "362d3d2e-91c9-4cbe-bb51-08827ff622a9"
      },
      "source": [
        "# Imports\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as tr\n",
        "\n",
        "# Models\n",
        "# from unet import Unet\n",
        "# from siamunet_conc import SiamUnet_conc\n",
        "# from siamunet_diff import SiamUnet_diff\n",
        "# from fresunet import FresUNet\n",
        "\n",
        "# Other\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from skimage import io\n",
        "from scipy.ndimage import zoom\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm as tqdm\n",
        "from pandas import read_csv\n",
        "from math import floor, ceil, sqrt, exp\n",
        "from IPython import display\n",
        "import time\n",
        "from itertools import chain\n",
        "import time\n",
        "import warnings\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "\n",
        "print('IMPORTS OK')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IMPORTS OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RPcV8BktCCW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7407be19-652a-4582-b8f7-df1bb3a97e37"
      },
      "source": [
        "# Global Variables' Definitions\n",
        "\n",
        "PATH_TO_DATASET = '/content/drive/My Drive/OneraDataset_Images/'\n",
        "IS_PROTOTYPE = False\n",
        "\n",
        "FP_MODIFIER = 10 # Tuning parameter, use 1 if unsure\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "PATCH_SIDE = 96\n",
        "N_EPOCHS = 50\n",
        "\n",
        "NORMALISE_IMGS = True\n",
        "\n",
        "TRAIN_STRIDE = int(PATCH_SIDE/2) - 1\n",
        "\n",
        "TYPE = 3 # 0-RGB | 1-RGBIr | 2-All bands s.t. resulution <= 20m | 3-All bands\n",
        "\n",
        "LOAD_TRAINED = False\n",
        "\n",
        "DATA_AUG = True\n",
        "\n",
        "\n",
        "print('DEFINITIONS OK')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEFINITIONS OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgI-4LV3tCCj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b289c15-997b-4ee5-9f3d-e84ec17b3615"
      },
      "source": [
        "# Functions\n",
        "\n",
        "def adjust_shape(I, s):\n",
        "    \"\"\"Adjust shape of grayscale image I to s.\"\"\"\n",
        "    \n",
        "    # crop if necesary\n",
        "    I = I[:s[0],:s[1]]\n",
        "    si = I.shape\n",
        "    \n",
        "    # pad if necessary \n",
        "    p0 = max(0,s[0] - si[0])\n",
        "    p1 = max(0,s[1] - si[1])\n",
        "    \n",
        "    return np.pad(I,((0,p0),(0,p1)),'edge')\n",
        "    \n",
        "\n",
        "def read_sentinel_img(path):\n",
        "    \"\"\"Read cropped Sentinel-2 image: RGB bands.\"\"\"\n",
        "    im_name = os.listdir(path)[0][:-7]\n",
        "    r = io.imread(path + im_name + \"B04.tif\")\n",
        "    g = io.imread(path + im_name + \"B03.tif\")\n",
        "    b = io.imread(path + im_name + \"B02.tif\")\n",
        "    \n",
        "    I = np.stack((r,g,b),axis=2).astype('float')\n",
        "    \n",
        "    if NORMALISE_IMGS:\n",
        "        I = (I - I.mean()) / I.std()\n",
        "\n",
        "    return I\n",
        "\n",
        "def read_sentinel_img_4(path):\n",
        "    \"\"\"Read cropped Sentinel-2 image: RGB and NIR bands.\"\"\"\n",
        "    im_name = os.listdir(path)[0][:-7]\n",
        "    r = io.imread(path + im_name + \"B04.tif\")\n",
        "    g = io.imread(path + im_name + \"B03.tif\")\n",
        "    b = io.imread(path + im_name + \"B02.tif\")\n",
        "    nir = io.imread(path + im_name + \"B08.tif\")\n",
        "    \n",
        "    I = np.stack((r,g,b,nir),axis=2).astype('float')\n",
        "    \n",
        "    if NORMALISE_IMGS:\n",
        "        I = (I - I.mean()) / I.std()\n",
        "\n",
        "    return I\n",
        "\n",
        "def read_sentinel_img_leq20(path):\n",
        "    \"\"\"Read cropped Sentinel-2 image: bands with resolution less than or equals to 20m.\"\"\"\n",
        "    im_name = os.listdir(path)[0][:-7]\n",
        "    \n",
        "    r = io.imread(path + im_name + \"B04.tif\")\n",
        "    s = r.shape\n",
        "    g = io.imread(path + im_name + \"B03.tif\")\n",
        "    b = io.imread(path + im_name + \"B02.tif\")\n",
        "    nir = io.imread(path + im_name + \"B08.tif\")\n",
        "    \n",
        "    ir1 = adjust_shape(zoom(io.imread(path + im_name + \"B05.tif\"),2),s)\n",
        "    ir2 = adjust_shape(zoom(io.imread(path + im_name + \"B06.tif\"),2),s)\n",
        "    ir3 = adjust_shape(zoom(io.imread(path + im_name + \"B07.tif\"),2),s)\n",
        "    nir2 = adjust_shape(zoom(io.imread(path + im_name + \"B8A.tif\"),2),s)\n",
        "    swir2 = adjust_shape(zoom(io.imread(path + im_name + \"B11.tif\"),2),s)\n",
        "    swir3 = adjust_shape(zoom(io.imread(path + im_name + \"B12.tif\"),2),s)\n",
        "    \n",
        "    I = np.stack((r,g,b,nir,ir1,ir2,ir3,nir2,swir2,swir3),axis=2).astype('float')\n",
        "    \n",
        "    if NORMALISE_IMGS:\n",
        "        I = (I - I.mean()) / I.std()\n",
        "\n",
        "    return I\n",
        "\n",
        "def read_sentinel_img_leq60(path):\n",
        "    \"\"\"Read cropped Sentinel-2 image: all bands.\"\"\"\n",
        "    im_name = os.listdir(path)[0][:-7]\n",
        "    \n",
        "    r = io.imread(path + im_name + \"B04.tif\")\n",
        "    s = r.shape\n",
        "    g = io.imread(path + im_name + \"B03.tif\")\n",
        "    b = io.imread(path + im_name + \"B02.tif\")\n",
        "    nir = io.imread(path + im_name + \"B08.tif\")\n",
        "    \n",
        "    ir1 = adjust_shape(zoom(io.imread(path + im_name + \"B05.tif\"),2),s)\n",
        "    ir2 = adjust_shape(zoom(io.imread(path + im_name + \"B06.tif\"),2),s)\n",
        "    ir3 = adjust_shape(zoom(io.imread(path + im_name + \"B07.tif\"),2),s)\n",
        "    nir2 = adjust_shape(zoom(io.imread(path + im_name + \"B8A.tif\"),2),s)\n",
        "    swir2 = adjust_shape(zoom(io.imread(path + im_name + \"B11.tif\"),2),s)\n",
        "    swir3 = adjust_shape(zoom(io.imread(path + im_name + \"B12.tif\"),2),s)\n",
        "    \n",
        "    uv = adjust_shape(zoom(io.imread(path + im_name + \"B01.tif\"),6),s)\n",
        "    wv = adjust_shape(zoom(io.imread(path + im_name + \"B09.tif\"),6),s)\n",
        "    swirc = adjust_shape(zoom(io.imread(path + im_name + \"B10.tif\"),6),s)\n",
        "    \n",
        "    I = np.stack((r,g,b,nir,ir1,ir2,ir3,nir2,swir2,swir3,uv,wv,swirc),axis=2).astype('float')\n",
        "    \n",
        "    if NORMALISE_IMGS:\n",
        "        I = (I - I.mean()) / I.std()\n",
        "\n",
        "    return I\n",
        "\n",
        "def read_sentinel_img_trio(path):\n",
        "    \"\"\"Read cropped Sentinel-2 image pair and change map.\"\"\"\n",
        "#     read images\n",
        "    if TYPE == 0:\n",
        "        I1 = read_sentinel_img(path + '/imgs_1/')\n",
        "        I2 = read_sentinel_img(path + '/imgs_2/')\n",
        "    elif TYPE == 1:\n",
        "        I1 = read_sentinel_img_4(path + '/imgs_1/')\n",
        "        I2 = read_sentinel_img_4(path + '/imgs_2/')\n",
        "    elif TYPE == 2:\n",
        "        I1 = read_sentinel_img_leq20(path + '/imgs_1/')\n",
        "        I2 = read_sentinel_img_leq20(path + '/imgs_2/')\n",
        "    elif TYPE == 3:\n",
        "        I1 = read_sentinel_img_leq60(path + '/imgs_1/')\n",
        "        I2 = read_sentinel_img_leq60(path + '/imgs_2/')\n",
        "        \n",
        "    cm = io.imread(path + '/cm/cm.png', as_gray=True) != 0\n",
        "    \n",
        "    # crop if necessary\n",
        "    s1 = I1.shape\n",
        "    s2 = I2.shape\n",
        "    I2 = np.pad(I2,((0, s1[0] - s2[0]), (0, s1[1] - s2[1]), (0,0)),'edge')\n",
        "    \n",
        "    \n",
        "    return I1, I2, cm\n",
        "\n",
        "\n",
        "\n",
        "def reshape_for_torch(I):\n",
        "    \"\"\"Transpose image for PyTorch coordinates.\"\"\"\n",
        "#     out = np.swapaxes(I,1,2)\n",
        "#     out = np.swapaxes(out,0,1)\n",
        "#     out = out[np.newaxis,:]\n",
        "    out = I.transpose((2, 0, 1))\n",
        "    return torch.from_numpy(out)\n",
        "\n",
        "\n",
        "\n",
        "class ChangeDetectionDataset(Dataset):\n",
        "    \"\"\"Change Detection dataset class, used for both training and test data.\"\"\"\n",
        "\n",
        "    def __init__(self, path, train = True, patch_side = 96, stride = None, use_all_bands = False, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        \n",
        "        # basics\n",
        "        self.transform = transform\n",
        "        self.path = path\n",
        "        self.patch_side = patch_side\n",
        "        if not stride:\n",
        "            self.stride = 1\n",
        "        else:\n",
        "            self.stride = stride\n",
        "        \n",
        "        if train:\n",
        "            fname = 'train.txt'\n",
        "        else:\n",
        "            fname = 'test.txt'\n",
        "        \n",
        "#         print(path + fname)\n",
        "        self.names = read_csv(path + fname).columns\n",
        "        self.n_imgs = self.names.shape[0]\n",
        "        \n",
        "        n_pix = 0\n",
        "        true_pix = 0\n",
        "        \n",
        "        \n",
        "        # load images\n",
        "        self.imgs_1 = {}\n",
        "        self.imgs_2 = {}\n",
        "        self.change_maps = {}\n",
        "        self.n_patches_per_image = {}\n",
        "        self.n_patches = 0\n",
        "        self.patch_coords = []\n",
        "        for im_name in tqdm(self.names):\n",
        "            # load and store each image\n",
        "            I1, I2, cm = read_sentinel_img_trio(self.path + im_name)\n",
        "            self.imgs_1[im_name] = reshape_for_torch(I1)\n",
        "            self.imgs_2[im_name] = reshape_for_torch(I2)\n",
        "            self.change_maps[im_name] = cm\n",
        "            \n",
        "            s = cm.shape\n",
        "            n_pix += np.prod(s)\n",
        "            true_pix += cm.sum()\n",
        "            \n",
        "            # calculate the number of patches\n",
        "            s = self.imgs_1[im_name].shape\n",
        "            n1 = ceil((s[1] - self.patch_side + 1) / self.stride)\n",
        "            n2 = ceil((s[2] - self.patch_side + 1) / self.stride)\n",
        "            n_patches_i = n1 * n2\n",
        "            self.n_patches_per_image[im_name] = n_patches_i\n",
        "            self.n_patches += n_patches_i\n",
        "            \n",
        "            # generate path coordinates\n",
        "            for i in range(n1):\n",
        "                for j in range(n2):\n",
        "                    # coordinates in (x1, x2, y1, y2)\n",
        "                    current_patch_coords = (im_name, \n",
        "                                    [self.stride*i, self.stride*i + self.patch_side, self.stride*j, self.stride*j + self.patch_side],\n",
        "                                    [self.stride*(i + 1), self.stride*(j + 1)])\n",
        "                    self.patch_coords.append(current_patch_coords)\n",
        "                    \n",
        "        self.weights = [ FP_MODIFIER * 2 * true_pix / n_pix, 2 * (n_pix - true_pix) / n_pix]\n",
        "        \n",
        "        \n",
        "\n",
        "    def get_img(self, im_name):\n",
        "        return self.imgs_1[im_name], self.imgs_2[im_name], self.change_maps[im_name]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_patches\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        current_patch_coords = self.patch_coords[idx]\n",
        "        im_name = current_patch_coords[0]\n",
        "        limits = current_patch_coords[1]\n",
        "        centre = current_patch_coords[2]\n",
        "        \n",
        "        I1 = self.imgs_1[im_name][:, limits[0]:limits[1], limits[2]:limits[3]]\n",
        "        I2 = self.imgs_2[im_name][:, limits[0]:limits[1], limits[2]:limits[3]]\n",
        "        \n",
        "        label = self.change_maps[im_name][limits[0]:limits[1], limits[2]:limits[3]]\n",
        "        label = torch.from_numpy(1*np.array(label)).float()\n",
        "        \n",
        "        sample = {'I1': I1, 'I2': I2, 'label': label}\n",
        "        \n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "\n",
        "class RandomFlip(object):\n",
        "    \"\"\"Flip randomly the images in a sample.\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         return\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
        "        \n",
        "        if random.random() > 0.5:\n",
        "            I1 =  I1.numpy()[:,:,::-1].copy()\n",
        "            I1 = torch.from_numpy(I1)\n",
        "            I2 =  I2.numpy()[:,:,::-1].copy()\n",
        "            I2 = torch.from_numpy(I2)\n",
        "            label =  label.numpy()[:,::-1].copy()\n",
        "            label = torch.from_numpy(label)\n",
        "\n",
        "        return {'I1': I1, 'I2': I2, 'label': label}\n",
        "\n",
        "\n",
        "\n",
        "class RandomRot(object):\n",
        "    \"\"\"Rotate randomly the images in a sample.\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         return\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
        "        \n",
        "        n = random.randint(0, 3)\n",
        "        if n:\n",
        "            I1 =  sample['I1'].numpy()\n",
        "            I1 = np.rot90(I1, n, axes=(1, 2)).copy()\n",
        "            I1 = torch.from_numpy(I1)\n",
        "            I2 =  sample['I2'].numpy()\n",
        "            I2 = np.rot90(I2, n, axes=(1, 2)).copy()\n",
        "            I2 = torch.from_numpy(I2)\n",
        "            label =  sample['label'].numpy()\n",
        "            label = np.rot90(label, n, axes=(0, 1)).copy()\n",
        "            label = torch.from_numpy(label)\n",
        "\n",
        "        return {'I1': I1, 'I2': I2, 'label': label}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('UTILS OK')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UTILS OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5l3hr1AtCCt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rDHwU5gtCC1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "bf324437-d27f-441a-9935-078e3d920268"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "\n",
        "if DATA_AUG:\n",
        "    data_transform = tr.Compose([RandomFlip(), RandomRot()])\n",
        "else:\n",
        "    data_transform = None\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "train_dataset = ChangeDetectionDataset(PATH_TO_DATASET, train = True, stride = TRAIN_STRIDE, transform=data_transform)\n",
        "weights = torch.FloatTensor(train_dataset.weights).cuda()\n",
        "print(weights)\n",
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)\n",
        "\n",
        "test_dataset = ChangeDetectionDataset(PATH_TO_DATASET, train = False, stride = TRAIN_STRIDE)\n",
        "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)\n",
        "\n",
        "\n",
        "print('DATASETS OK')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [02:43<00:00, 11.69s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([0.4595, 1.9540], device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:45<00:00, 10.55s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATASETS OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s94wymZtCDD"
      },
      "source": [
        "# print(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04EENAksE1Zd"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.padding import ReplicationPad2d\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    \"\"\"EF segmentation network.\"\"\"\n",
        "\n",
        "    def __init__(self, input_nbr, label_nbr):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        self.input_nbr = input_nbr\n",
        "        self.pad = nn.ReflectionPad2d(1)\n",
        "\n",
        "        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n",
        "        self.conv11_ = nn.Conv2d(input_nbr, 16, kernel_size=5, padding=1)\n",
        "        self.bn11 = nn.BatchNorm2d(16)\n",
        "        self.do11 = nn.Dropout2d(p=0.2)\n",
        "        self.conv12 = nn.Conv2d(16*2, 16, kernel_size=3, padding=1)\n",
        "        self.conv12_ = nn.Conv2d(16, 16, kernel_size=5, padding=1)\n",
        "        self.bn12 = nn.BatchNorm2d(16)\n",
        "        self.do12 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.conv21_ = nn.Conv2d(16, 32, kernel_size=5, padding=1)\n",
        "        self.bn21 = nn.BatchNorm2d(32)\n",
        "        self.do21 = nn.Dropout2d(p=0.2)\n",
        "        self.conv22 = nn.Conv2d(32*2, 32, kernel_size=3, padding=1)\n",
        "        self.conv22_ = nn.Conv2d(32, 32, kernel_size=5, padding=1)\n",
        "        self.bn22 = nn.BatchNorm2d(32)\n",
        "        self.do22 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv31_ = nn.Conv2d(32, 64, kernel_size=5, padding=1)\n",
        "        self.bn31 = nn.BatchNorm2d(64)\n",
        "        self.do31 = nn.Dropout2d(p=0.2)\n",
        "        self.conv32 = nn.Conv2d(64*2, 64, kernel_size=3, padding=1)\n",
        "        self.conv32_ = nn.Conv2d(64*2, 64, kernel_size=5, padding=1)\n",
        "        self.bn32 = nn.BatchNorm2d(64)\n",
        "        self.do32 = nn.Dropout2d(p=0.2)\n",
        "        self.conv33 = nn.Conv2d(64*2, 64, kernel_size=3, padding=1)\n",
        "        self.conv33_ = nn.Conv2d(64, 64, kernel_size=5, padding=1)\n",
        "        self.bn33 = nn.BatchNorm2d(64)\n",
        "        self.do33 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv41_ = nn.Conv2d(64, 128, kernel_size=5, padding=1)\n",
        "        self.bn41 = nn.BatchNorm2d(128)\n",
        "        self.do41 = nn.Dropout2d(p=0.2)\n",
        "        self.conv42 = nn.Conv2d(128*2, 128, kernel_size=3, padding=1)\n",
        "        self.conv42_ = nn.Conv2d(128*2, 128, kernel_size=5, padding=1)\n",
        "        self.bn42 = nn.BatchNorm2d(128)\n",
        "        self.do42 = nn.Dropout2d(p=0.2)\n",
        "        self.conv43 = nn.Conv2d(128*2, 128, kernel_size=3, padding=1)\n",
        "        self.conv43_ = nn.Conv2d(128, 128, kernel_size=5, padding=1)\n",
        "        self.bn43 = nn.BatchNorm2d(128)\n",
        "        self.do43 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n",
        "        self.bn43d = nn.BatchNorm2d(128)\n",
        "        self.do43d = nn.Dropout2d(p=0.2)\n",
        "        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.bn42d = nn.BatchNorm2d(128)\n",
        "        self.do42d = nn.Dropout2d(p=0.2)\n",
        "        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn41d = nn.BatchNorm2d(64)\n",
        "        self.do41d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
        "        self.bn33d = nn.BatchNorm2d(64)\n",
        "        self.do33d = nn.Dropout2d(p=0.2)\n",
        "        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.bn32d = nn.BatchNorm2d(64)\n",
        "        self.do32d = nn.Dropout2d(p=0.2)\n",
        "        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn31d = nn.BatchNorm2d(32)\n",
        "        self.do31d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
        "        self.bn22d = nn.BatchNorm2d(32)\n",
        "        self.do22d = nn.Dropout2d(p=0.2)\n",
        "        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn21d = nn.BatchNorm2d(16)\n",
        "        self.do21d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
        "        self.bn12d = nn.BatchNorm2d(16)\n",
        "        self.do12d = nn.Dropout2d(p=0.2)\n",
        "        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n",
        "\n",
        "        self.sm = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "\n",
        "        x = torch.cat((x1, x2), 1)\n",
        "\n",
        "        \"\"\"Forward method.\"\"\"\n",
        "        # Stage 1\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x))))\n",
        "        x11_ = self.do11(F.relu(self.bn11(self.conv11_(self.pad(x)))))\n",
        "        x11 = torch.cat((x11,x11_), dim=1)\n",
        "        x12 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
        "        # x12_ = self.do12(F.relu(self.bn12(self.conv12_(self.pad(x11)))))\n",
        "        # x12 = torch.cat((x12,x12_), dim=1)\n",
        "        x1p = F.max_pool2d(x12, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 2\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
        "        x21_ = self.do21(F.relu(self.bn21(self.conv21_(self.pad(x1p)))))\n",
        "        x21 = torch.cat((x21,x21_), dim=1)\n",
        "        x22 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
        "        # x22_ = self.do22(F.relu(self.bn22(self.conv22_(self.pad(x21)))))\n",
        "        # x22 = torch.cat((x22,x22_), dim=1)\n",
        "        x2p = F.max_pool2d(x22, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 3\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
        "        x31_ = self.do31(F.relu(self.bn31(self.conv31_(self.pad(x2p)))))\n",
        "        x31 = torch.cat((x31,x31_), dim=1)\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
        "        x32_ = self.do32(F.relu(self.bn32(self.conv32_(self.pad(x31)))))\n",
        "        x32 = torch.cat((x32,x32_), dim=1)\n",
        "        x33 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
        "        # x33_ = self.do33(F.relu(self.bn33(self.conv33_(self.pad(x32)))))\n",
        "        # x33 = torch.cat((x33,x33_), dim=1)\n",
        "        x3p = F.max_pool2d(x33, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
        "        x41_ = self.do41(F.relu(self.bn41(self.conv41_(self.pad(x3p)))))\n",
        "        x41 = torch.cat((x41,x41_), dim=1)\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
        "        x42_ = self.do42(F.relu(self.bn42(self.conv42_(self.pad(x41)))))\n",
        "        x42 = torch.cat((x42,x42_), dim=1)\n",
        "        x43 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
        "        # x43_ = self.do43(F.relu(self.bn43(self.conv43_(self.pad(x42)))))\n",
        "        # x43 = torch.cat((x43,x43_), dim=1)\n",
        "        x4p = F.max_pool2d(x43, kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "        # Stage 4d\n",
        "        x4d = self.upconv4(x4p)\n",
        "        pad4 = ReplicationPad2d((0, x43.size(3) - x4d.size(3), 0, x43.size(2) - x4d.size(2)))\n",
        "        x4d = torch.cat((pad4(x4d), x43), 1)\n",
        "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n",
        "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n",
        "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n",
        "\n",
        "        # Stage 3d\n",
        "        x3d = self.upconv3(x41d)\n",
        "        pad3 = ReplicationPad2d((0, x33.size(3) - x3d.size(3), 0, x33.size(2) - x3d.size(2)))\n",
        "        x3d = torch.cat((pad3(x3d), x33), 1)\n",
        "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n",
        "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n",
        "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n",
        "\n",
        "        # Stage 2d\n",
        "        x2d = self.upconv2(x31d)\n",
        "        pad2 = ReplicationPad2d((0, x22.size(3) - x2d.size(3), 0, x22.size(2) - x2d.size(2)))\n",
        "        x2d = torch.cat((pad2(x2d), x22), 1)\n",
        "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n",
        "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n",
        "\n",
        "        # Stage 1d\n",
        "        x1d = self.upconv1(x21d)\n",
        "        pad1 = ReplicationPad2d((0, x12.size(3) - x1d.size(3), 0, x12.size(2) - x1d.size(2)))\n",
        "        x1d = torch.cat((pad1(x1d), x12), 1)\n",
        "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n",
        "        x11d = self.conv11d(x12d)\n",
        "\n",
        "        return self.sm(x11d)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_-6nJdHtCDL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "079221c8-df57-45a9-86c3-6076c082b7e1"
      },
      "source": [
        "# 0-RGB | 1-RGBIr | 2-All bands s.t. resulution <= 20m | 3-All bands\n",
        "\n",
        "if TYPE == 0:\n",
        "    net, net_name = Unet(2*3, 2), 'FC-EF'\n",
        "#     net, net_name = SiamUnet_conc(3, 2), 'FC-Siam-conc'\n",
        "#     net, net_name = SiamUnet_diff(3, 2), 'FC-Siam-diff'\n",
        "    # net, net_name = FresUNet(2*3, 2), 'FresUNet'\n",
        "elif TYPE == 1:\n",
        "    net, net_name = Unet(2*4, 2), 'FC-EF'\n",
        "#     net, net_name = SiamUnet_conc(4, 2), 'FC-Siam-conc'\n",
        "#     net, net_name = SiamUnet_diff(4, 2), 'FC-Siam-diff'\n",
        "    # net, net_name = FresUNet(2*4, 2), 'FresUNet'\n",
        "elif TYPE == 2:\n",
        "    net, net_name = Unet(2*10, 2), 'FC-EF'\n",
        "#     net, net_name = SiamUnet_conc(10, 2), 'FC-Siam-conc'\n",
        "#     net, net_name = SiamUnet_diff(10, 2), 'FC-Siam-diff'\n",
        "    # net, net_name = FresUNet(2*10, 2), 'FresUNet'\n",
        "elif TYPE == 3:\n",
        "    net, net_name = Unet(2*13, 2), 'FC-EF'\n",
        "#     net, net_name = SiamUnet_conc(13, 2), 'FC-Siam-conc'\n",
        "#     net, net_name = SiamUnet_diff(13, 2), 'FC-Siam-diff'\n",
        "    # net, net_name = FresUNet(2*13, 2), 'FresUNet'\n",
        "\n",
        "\n",
        "net.cuda()\n",
        "\n",
        "criterion = nn.NLLLoss(weight=weights) # to be used with logsoftmax output\n",
        "from torchsummary import summary\n",
        "\n",
        "summary(net,input_size=[(13,256,256),(13,256,256)])\n",
        "\n",
        "print('NETWORK OK')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 256, 256]           3,760\n",
            "       BatchNorm2d-2         [-1, 16, 256, 256]              32\n",
            "         Dropout2d-3         [-1, 16, 256, 256]               0\n",
            "   ReflectionPad2d-4         [-1, 26, 258, 258]               0\n",
            "            Conv2d-5         [-1, 16, 256, 256]          10,416\n",
            "       BatchNorm2d-6         [-1, 16, 256, 256]              32\n",
            "         Dropout2d-7         [-1, 16, 256, 256]               0\n",
            "            Conv2d-8         [-1, 16, 256, 256]           4,624\n",
            "       BatchNorm2d-9         [-1, 16, 256, 256]              32\n",
            "        Dropout2d-10         [-1, 16, 256, 256]               0\n",
            "           Conv2d-11         [-1, 32, 128, 128]           4,640\n",
            "      BatchNorm2d-12         [-1, 32, 128, 128]              64\n",
            "        Dropout2d-13         [-1, 32, 128, 128]               0\n",
            "  ReflectionPad2d-14         [-1, 16, 130, 130]               0\n",
            "           Conv2d-15         [-1, 32, 128, 128]          12,832\n",
            "      BatchNorm2d-16         [-1, 32, 128, 128]              64\n",
            "        Dropout2d-17         [-1, 32, 128, 128]               0\n",
            "           Conv2d-18         [-1, 32, 128, 128]          18,464\n",
            "      BatchNorm2d-19         [-1, 32, 128, 128]              64\n",
            "        Dropout2d-20         [-1, 32, 128, 128]               0\n",
            "           Conv2d-21           [-1, 64, 64, 64]          18,496\n",
            "      BatchNorm2d-22           [-1, 64, 64, 64]             128\n",
            "        Dropout2d-23           [-1, 64, 64, 64]               0\n",
            "  ReflectionPad2d-24           [-1, 32, 66, 66]               0\n",
            "           Conv2d-25           [-1, 64, 64, 64]          51,264\n",
            "      BatchNorm2d-26           [-1, 64, 64, 64]             128\n",
            "        Dropout2d-27           [-1, 64, 64, 64]               0\n",
            "           Conv2d-28           [-1, 64, 64, 64]          73,792\n",
            "      BatchNorm2d-29           [-1, 64, 64, 64]             128\n",
            "        Dropout2d-30           [-1, 64, 64, 64]               0\n",
            "  ReflectionPad2d-31          [-1, 128, 66, 66]               0\n",
            "           Conv2d-32           [-1, 64, 64, 64]         204,864\n",
            "      BatchNorm2d-33           [-1, 64, 64, 64]             128\n",
            "        Dropout2d-34           [-1, 64, 64, 64]               0\n",
            "           Conv2d-35           [-1, 64, 64, 64]          73,792\n",
            "      BatchNorm2d-36           [-1, 64, 64, 64]             128\n",
            "        Dropout2d-37           [-1, 64, 64, 64]               0\n",
            "           Conv2d-38          [-1, 128, 32, 32]          73,856\n",
            "      BatchNorm2d-39          [-1, 128, 32, 32]             256\n",
            "        Dropout2d-40          [-1, 128, 32, 32]               0\n",
            "  ReflectionPad2d-41           [-1, 64, 34, 34]               0\n",
            "           Conv2d-42          [-1, 128, 32, 32]         204,928\n",
            "      BatchNorm2d-43          [-1, 128, 32, 32]             256\n",
            "        Dropout2d-44          [-1, 128, 32, 32]               0\n",
            "           Conv2d-45          [-1, 128, 32, 32]         295,040\n",
            "      BatchNorm2d-46          [-1, 128, 32, 32]             256\n",
            "        Dropout2d-47          [-1, 128, 32, 32]               0\n",
            "  ReflectionPad2d-48          [-1, 256, 34, 34]               0\n",
            "           Conv2d-49          [-1, 128, 32, 32]         819,328\n",
            "      BatchNorm2d-50          [-1, 128, 32, 32]             256\n",
            "        Dropout2d-51          [-1, 128, 32, 32]               0\n",
            "           Conv2d-52          [-1, 128, 32, 32]         295,040\n",
            "      BatchNorm2d-53          [-1, 128, 32, 32]             256\n",
            "        Dropout2d-54          [-1, 128, 32, 32]               0\n",
            "  ConvTranspose2d-55          [-1, 128, 32, 32]         147,584\n",
            "  ConvTranspose2d-56          [-1, 128, 32, 32]         295,040\n",
            "      BatchNorm2d-57          [-1, 128, 32, 32]             256\n",
            "        Dropout2d-58          [-1, 128, 32, 32]               0\n",
            "  ConvTranspose2d-59          [-1, 128, 32, 32]         147,584\n",
            "      BatchNorm2d-60          [-1, 128, 32, 32]             256\n",
            "        Dropout2d-61          [-1, 128, 32, 32]               0\n",
            "  ConvTranspose2d-62           [-1, 64, 32, 32]          73,792\n",
            "      BatchNorm2d-63           [-1, 64, 32, 32]             128\n",
            "        Dropout2d-64           [-1, 64, 32, 32]               0\n",
            "  ConvTranspose2d-65           [-1, 64, 64, 64]          36,928\n",
            "  ConvTranspose2d-66           [-1, 64, 64, 64]          73,792\n",
            "      BatchNorm2d-67           [-1, 64, 64, 64]             128\n",
            "        Dropout2d-68           [-1, 64, 64, 64]               0\n",
            "  ConvTranspose2d-69           [-1, 64, 64, 64]          36,928\n",
            "      BatchNorm2d-70           [-1, 64, 64, 64]             128\n",
            "        Dropout2d-71           [-1, 64, 64, 64]               0\n",
            "  ConvTranspose2d-72           [-1, 32, 64, 64]          18,464\n",
            "      BatchNorm2d-73           [-1, 32, 64, 64]              64\n",
            "        Dropout2d-74           [-1, 32, 64, 64]               0\n",
            "  ConvTranspose2d-75         [-1, 32, 128, 128]           9,248\n",
            "  ConvTranspose2d-76         [-1, 32, 128, 128]          18,464\n",
            "      BatchNorm2d-77         [-1, 32, 128, 128]              64\n",
            "        Dropout2d-78         [-1, 32, 128, 128]               0\n",
            "  ConvTranspose2d-79         [-1, 16, 128, 128]           4,624\n",
            "      BatchNorm2d-80         [-1, 16, 128, 128]              32\n",
            "        Dropout2d-81         [-1, 16, 128, 128]               0\n",
            "  ConvTranspose2d-82         [-1, 16, 256, 256]           2,320\n",
            "  ConvTranspose2d-83         [-1, 16, 256, 256]           4,624\n",
            "      BatchNorm2d-84         [-1, 16, 256, 256]              32\n",
            "        Dropout2d-85         [-1, 16, 256, 256]               0\n",
            "  ConvTranspose2d-86          [-1, 2, 256, 256]             290\n",
            "       LogSoftmax-87          [-1, 2, 256, 256]               0\n",
            "================================================================\n",
            "Total params: 3,038,114\n",
            "Trainable params: 3,038,114\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 2768896.00\n",
            "Forward/backward pass size (MB): 257.91\n",
            "Params size (MB): 11.59\n",
            "Estimated Total Size (MB): 2769165.50\n",
            "----------------------------------------------------------------\n",
            "NETWORK OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeLsY037tCDT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8173e1e-96da-4be6-c8b2-52c427dd83b6"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('Number of trainable parameters:', count_parameters(net))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of trainable parameters: 3581490\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7dfDWGCtCDb"
      },
      "source": [
        "# net.load_state_dict(torch.load('net-best_epoch-1_fm-0.7394933126157746.pth.tar'))\n",
        "\n",
        "def train(n_epochs = N_EPOCHS, save = True):\n",
        "    t = np.linspace(1, n_epochs, n_epochs)\n",
        "    \n",
        "    epoch_train_loss = 0 * t\n",
        "    epoch_train_accuracy = 0 * t\n",
        "    epoch_train_change_accuracy = 0 * t\n",
        "    epoch_train_nochange_accuracy = 0 * t\n",
        "    epoch_train_precision = 0 * t\n",
        "    epoch_train_recall = 0 * t\n",
        "    epoch_train_Fmeasure = 0 * t\n",
        "    epoch_test_loss = 0 * t\n",
        "    epoch_test_accuracy = 0 * t\n",
        "    epoch_test_change_accuracy = 0 * t\n",
        "    epoch_test_nochange_accuracy = 0 * t\n",
        "    epoch_test_precision = 0 * t\n",
        "    epoch_test_recall = 0 * t\n",
        "    epoch_test_Fmeasure = 0 * t\n",
        "    \n",
        "#     mean_acc = 0\n",
        "#     best_mean_acc = 0\n",
        "    fm = 0\n",
        "    best_fm = 0\n",
        "    \n",
        "    lss = 1000\n",
        "    best_lss = 1000\n",
        "    \n",
        "    plt.figure(num=1)\n",
        "    plt.figure(num=2)\n",
        "    plt.figure(num=3)\n",
        "    \n",
        "    \n",
        "    optimizer = torch.optim.Adam(net.parameters(), weight_decay=1e-4)\n",
        "#     optimizer = torch.optim.Adam(net.parameters(), lr=0.0005)\n",
        "        \n",
        "    \n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n",
        "    \n",
        "    \n",
        "    for epoch_index in tqdm(range(n_epochs)):\n",
        "        net.train()\n",
        "        print('Epoch: ' + str(epoch_index + 1) + ' of ' + str(N_EPOCHS))\n",
        "\n",
        "        tot_count = 0\n",
        "        tot_loss = 0\n",
        "        tot_accurate = 0\n",
        "        class_correct = list(0. for i in range(2))\n",
        "        class_total = list(0. for i in range(2))\n",
        "#         for batch_index, batch in enumerate(tqdm(data_loader)):\n",
        "        for batch in train_loader:\n",
        "            I1 = Variable(batch['I1'].float().cuda())\n",
        "            I2 = Variable(batch['I2'].float().cuda())\n",
        "            label = torch.squeeze(Variable(batch['label'].cuda()))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = net(I1, I2)\n",
        "            loss = criterion(output, label.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "        epoch_train_loss[epoch_index], epoch_train_accuracy[epoch_index], cl_acc, pr_rec = test(train_dataset)\n",
        "        epoch_train_nochange_accuracy[epoch_index] = cl_acc[0]\n",
        "        epoch_train_change_accuracy[epoch_index] = cl_acc[1]\n",
        "        epoch_train_precision[epoch_index] = pr_rec[0]\n",
        "        epoch_train_recall[epoch_index] = pr_rec[1]\n",
        "        epoch_train_Fmeasure[epoch_index] = pr_rec[2]\n",
        "        \n",
        "#         epoch_test_loss[epoch_index], epoch_test_accuracy[epoch_index], cl_acc, pr_rec = test(test_dataset)\n",
        "        epoch_test_loss[epoch_index], epoch_test_accuracy[epoch_index], cl_acc, pr_rec = test(test_dataset)\n",
        "        epoch_test_nochange_accuracy[epoch_index] = cl_acc[0]\n",
        "        epoch_test_change_accuracy[epoch_index] = cl_acc[1]\n",
        "        epoch_test_precision[epoch_index] = pr_rec[0]\n",
        "        epoch_test_recall[epoch_index] = pr_rec[1]\n",
        "        epoch_test_Fmeasure[epoch_index] = pr_rec[2]\n",
        "\n",
        "        plt.figure(num=1)\n",
        "        plt.clf()\n",
        "        l1_1, = plt.plot(t[:epoch_index + 1], epoch_train_loss[:epoch_index + 1], label='Train loss')\n",
        "        l1_2, = plt.plot(t[:epoch_index + 1], epoch_test_loss[:epoch_index + 1], label='Test loss')\n",
        "        plt.legend(handles=[l1_1, l1_2])\n",
        "        plt.grid()\n",
        "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
        "        plt.gcf().gca().set_xlim(left = 0)\n",
        "        plt.title('Loss')\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "        plt.figure(num=2)\n",
        "        plt.clf()\n",
        "        l2_1, = plt.plot(t[:epoch_index + 1], epoch_train_accuracy[:epoch_index + 1], label='Train accuracy')\n",
        "        l2_2, = plt.plot(t[:epoch_index + 1], epoch_test_accuracy[:epoch_index + 1], label='Test accuracy')\n",
        "        plt.legend(handles=[l2_1, l2_2])\n",
        "        plt.grid()\n",
        "        plt.gcf().gca().set_ylim(0, 100)\n",
        "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
        "#         plt.gcf().gca().set_xlim(left = 0)\n",
        "        plt.title('Accuracy')\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "        plt.figure(num=3)\n",
        "        plt.clf()\n",
        "        l3_1, = plt.plot(t[:epoch_index + 1], epoch_train_nochange_accuracy[:epoch_index + 1], label='Train accuracy: no change')\n",
        "        l3_2, = plt.plot(t[:epoch_index + 1], epoch_train_change_accuracy[:epoch_index + 1], label='Train accuracy: change')\n",
        "        l3_3, = plt.plot(t[:epoch_index + 1], epoch_test_nochange_accuracy[:epoch_index + 1], label='Test accuracy: no change')\n",
        "        l3_4, = plt.plot(t[:epoch_index + 1], epoch_test_change_accuracy[:epoch_index + 1], label='Test accuracy: change')\n",
        "        plt.legend(handles=[l3_1, l3_2, l3_3, l3_4])\n",
        "        plt.grid()\n",
        "        plt.gcf().gca().set_ylim(0, 100)\n",
        "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
        "#         plt.gcf().gca().set_xlim(left = 0)\n",
        "        plt.title('Accuracy per class')\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "        plt.figure(num=4)\n",
        "        plt.clf()\n",
        "        l4_1, = plt.plot(t[:epoch_index + 1], epoch_train_precision[:epoch_index + 1], label='Train precision')\n",
        "        l4_2, = plt.plot(t[:epoch_index + 1], epoch_train_recall[:epoch_index + 1], label='Train recall')\n",
        "        l4_3, = plt.plot(t[:epoch_index + 1], epoch_train_Fmeasure[:epoch_index + 1], label='Train Dice/F1')\n",
        "        l4_4, = plt.plot(t[:epoch_index + 1], epoch_test_precision[:epoch_index + 1], label='Test precision')\n",
        "        l4_5, = plt.plot(t[:epoch_index + 1], epoch_test_recall[:epoch_index + 1], label='Test recall')\n",
        "        l4_6, = plt.plot(t[:epoch_index + 1], epoch_test_Fmeasure[:epoch_index + 1], label='Test Dice/F1')\n",
        "        plt.legend(handles=[l4_1, l4_2, l4_3, l4_4, l4_5, l4_6])\n",
        "        plt.grid()\n",
        "        plt.gcf().gca().set_ylim(0, 1)\n",
        "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
        "#         plt.gcf().gca().set_xlim(left = 0)\n",
        "        plt.title('Precision, Recall and F-measure')\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "        \n",
        "        \n",
        "#         mean_acc = (epoch_test_nochange_accuracy[epoch_index] + epoch_test_change_accuracy[epoch_index])/2\n",
        "#         if mean_acc > best_mean_acc:\n",
        "#             best_mean_acc = mean_acc\n",
        "#             save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_acc-' + str(mean_acc) + '.pth.tar'\n",
        "#             torch.save(net.state_dict(), save_str)\n",
        "        \n",
        "        \n",
        "#         fm = pr_rec[2]\n",
        "        fm = epoch_train_Fmeasure[epoch_index]\n",
        "        if fm > best_fm:\n",
        "            best_fm = fm\n",
        "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_fm-' + str(fm) + '.pth.tar'\n",
        "            torch.save(net.state_dict(), save_str)\n",
        "        \n",
        "        lss = epoch_train_loss[epoch_index]\n",
        "        if lss < best_lss:\n",
        "            best_lss = lss\n",
        "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_loss-' + str(lss) + '.pth.tar'\n",
        "            torch.save(net.state_dict(), save_str)\n",
        "            \n",
        "            \n",
        "#         print('Epoch loss: ' + str(tot_loss/tot_count))\n",
        "        if save:\n",
        "            im_format = 'png'\n",
        "    #         im_format = 'eps'\n",
        "\n",
        "            plt.figure(num=1)\n",
        "            plt.savefig(net_name + '-01-loss.' + im_format)\n",
        "\n",
        "            plt.figure(num=2)\n",
        "            plt.savefig(net_name + '-02-accuracy.' + im_format)\n",
        "\n",
        "            plt.figure(num=3)\n",
        "            plt.savefig(net_name + '-03-accuracy-per-class.' + im_format)\n",
        "\n",
        "            plt.figure(num=4)\n",
        "            plt.savefig(net_name + '-04-prec-rec-fmeas.' + im_format)\n",
        "        \n",
        "    out = {'train_loss': epoch_train_loss[-1],\n",
        "           'train_accuracy': epoch_train_accuracy[-1],\n",
        "           'train_nochange_accuracy': epoch_train_nochange_accuracy[-1],\n",
        "           'train_change_accuracy': epoch_train_change_accuracy[-1],\n",
        "           'test_loss': epoch_test_loss[-1],\n",
        "           'test_accuracy': epoch_test_accuracy[-1],\n",
        "           'test_nochange_accuracy': epoch_test_nochange_accuracy[-1],\n",
        "           'test_change_accuracy': epoch_test_change_accuracy[-1]}\n",
        "    \n",
        "    print('pr_c, rec_c, f_meas, pr_nc, rec_nc')\n",
        "    print(pr_rec)\n",
        "    \n",
        "    return out\n",
        "\n",
        "L = 1024\n",
        "N = 2\n",
        "\n",
        "def test(dset):\n",
        "    net.eval()\n",
        "    tot_loss = 0\n",
        "    tot_count = 0\n",
        "    tot_accurate = 0\n",
        "    \n",
        "    n = 2\n",
        "    class_correct = list(0. for i in range(n))\n",
        "    class_total = list(0. for i in range(n))\n",
        "    class_accuracy = list(0. for i in range(n))\n",
        "    \n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    \n",
        "    for img_index in dset.names:\n",
        "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
        "        \n",
        "        s = cm_full.shape\n",
        "        \n",
        "\n",
        "        steps0 = np.arange(0,s[0],ceil(s[0]/N))\n",
        "        steps1 = np.arange(0,s[1],ceil(s[1]/N))\n",
        "        for ii in range(N):\n",
        "            for jj in range(N):\n",
        "                xmin = steps0[ii]\n",
        "                if ii == N-1:\n",
        "                    xmax = s[0]\n",
        "                else:\n",
        "                    xmax = steps0[ii+1]\n",
        "                ymin = jj\n",
        "                if jj == N-1:\n",
        "                    ymax = s[1]\n",
        "                else:\n",
        "                    ymax = steps1[jj+1]\n",
        "                I1 = I1_full[:, xmin:xmax, ymin:ymax]\n",
        "                I2 = I2_full[:, xmin:xmax, ymin:ymax]\n",
        "                cm = cm_full[xmin:xmax, ymin:ymax]\n",
        "\n",
        "                I1 = Variable(torch.unsqueeze(I1, 0).float()).cuda()\n",
        "                I2 = Variable(torch.unsqueeze(I2, 0).float()).cuda()\n",
        "                cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm),0).float()).cuda()\n",
        "\n",
        "\n",
        "                output = net(I1, I2)\n",
        "                loss = criterion(output, cm.long())\n",
        "        #         print(loss)\n",
        "                tot_loss += loss.data * np.prod(cm.size())\n",
        "                tot_count += np.prod(cm.size())\n",
        "\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "                c = (predicted.int() == cm.data.int())\n",
        "                for i in range(c.size(1)):\n",
        "                    for j in range(c.size(2)):\n",
        "                        l = int(cm.data[0, i, j])\n",
        "                        class_correct[l] += c[0, i, j]\n",
        "                        class_total[l] += 1\n",
        "                        \n",
        "                pr = (predicted.int() > 0).cpu().numpy()\n",
        "                gt = (cm.data.int() > 0).cpu().numpy()\n",
        "                \n",
        "                tp += np.logical_and(pr, gt).sum()\n",
        "                tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
        "                fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
        "                fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
        "        \n",
        "    net_loss = tot_loss/tot_count\n",
        "    net_accuracy = 100 * (tp + tn)/tot_count\n",
        "    \n",
        "    for i in range(n):\n",
        "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
        "\n",
        "    prec = tp / (tp + fp)\n",
        "    rec = tp / (tp + fn)\n",
        "    f_meas = 2 * prec * rec / (prec + rec)\n",
        "    prec_nc = tn / (tn + fn)\n",
        "    rec_nc = tn / (tn + fp)\n",
        "    \n",
        "    pr_rec = [prec, rec, f_meas, prec_nc, rec_nc]\n",
        "        \n",
        "    return net_loss, net_accuracy, class_accuracy, pr_rec\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q2B2ZsbtCDi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "9d00053a-5a29-466d-9aeb-397f0348ca70"
      },
      "source": [
        "if LOAD_TRAINED:\n",
        "    net.load_state_dict(torch.load('net_final.pth.tar'))\n",
        "    print('LOAD OK')\n",
        "else:\n",
        "    t_start = time.time()\n",
        "    out_dic = train()\n",
        "    t_end = time.time()\n",
        "    print(out_dic)\n",
        "    print('Elapsed time:')\n",
        "    print(t_end - t_start)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 2/50 [17:59<7:10:39, 538.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 3 of 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpYtA84qtCDp"
      },
      "source": [
        "if not LOAD_TRAINED:\n",
        "    torch.save(net.state_dict(), 'net_final.pth.tar')\n",
        "    print('SAVE OK')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4icwmptgtCDv"
      },
      "source": [
        "\n",
        "\n",
        "def save_test_results(dset):\n",
        "    for name in tqdm(dset.names):\n",
        "        with warnings.catch_warnings():\n",
        "            I1, I2, cm = dset.get_img(name)\n",
        "            I1 = Variable(torch.unsqueeze(I1, 0).float()).cuda()\n",
        "            I2 = Variable(torch.unsqueeze(I2, 0).float()).cuda()\n",
        "            out = net(I1, I2)\n",
        "            _, predicted = torch.max(out.data, 1)\n",
        "            I = np.stack((255*cm,255*np.squeeze(predicted.cpu().numpy()),255*cm),2)\n",
        "            io.imsave(f'{net_name}-{name}.png',I)\n",
        "\n",
        "\n",
        "\n",
        "t_start = time.time()\n",
        "# save_test_results(train_dataset)\n",
        "save_test_results(test_dataset)\n",
        "t_end = time.time()\n",
        "print('Elapsed time: {}'.format(t_end - t_start))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3peRWNtxtCD0"
      },
      "source": [
        "L = 1024\n",
        "\n",
        "def kappa(tp, tn, fp, fn):\n",
        "    N = tp + tn + fp + fn\n",
        "    p0 = (tp + tn) / N\n",
        "    pe = ((tp+fp)*(tp+fn) + (tn+fp)*(tn+fn)) / (N * N)\n",
        "    \n",
        "    return (p0 - pe) / (1 - pe)\n",
        "\n",
        "def test(dset):\n",
        "    net.eval()\n",
        "    tot_loss = 0\n",
        "    tot_count = 0\n",
        "    tot_accurate = 0\n",
        "    \n",
        "    n = 2\n",
        "    class_correct = list(0. for i in range(n))\n",
        "    class_total = list(0. for i in range(n))\n",
        "    class_accuracy = list(0. for i in range(n))\n",
        "    \n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    \n",
        "    for img_index in tqdm(dset.names):\n",
        "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
        "        \n",
        "        s = cm_full.shape\n",
        "        \n",
        "        for ii in range(ceil(s[0]/L)):\n",
        "            for jj in range(ceil(s[1]/L)):\n",
        "                xmin = L*ii\n",
        "                xmax = min(L*(ii+1),s[1])\n",
        "                ymin = L*jj\n",
        "                ymax = min(L*(jj+1),s[1])\n",
        "                I1 = I1_full[:, xmin:xmax, ymin:ymax]\n",
        "                I2 = I2_full[:, xmin:xmax, ymin:ymax]\n",
        "                cm = cm_full[xmin:xmax, ymin:ymax]\n",
        "\n",
        "                I1 = Variable(torch.unsqueeze(I1, 0).float()).cuda()\n",
        "                I2 = Variable(torch.unsqueeze(I2, 0).float()).cuda()\n",
        "                cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm),0).float()).cuda()\n",
        "\n",
        "                output = net(I1, I2)\n",
        "                    \n",
        "                loss = criterion(output, cm.long())\n",
        "                tot_loss += loss.data * np.prod(cm.size())\n",
        "                tot_count += np.prod(cm.size())\n",
        "\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "                c = (predicted.int() == cm.data.int())\n",
        "                for i in range(c.size(1)):\n",
        "                    for j in range(c.size(2)):\n",
        "                        l = int(cm.data[0, i, j])\n",
        "                        class_correct[l] += c[0, i, j]\n",
        "                        class_total[l] += 1\n",
        "                        \n",
        "                pr = (predicted.int() > 0).cpu().numpy()\n",
        "                gt = (cm.data.int() > 0).cpu().numpy()\n",
        "                \n",
        "                tp += np.logical_and(pr, gt).sum()\n",
        "                tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
        "                fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
        "                fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
        "        \n",
        "    net_loss = tot_loss/tot_count        \n",
        "    net_loss = float(net_loss.cpu().numpy())\n",
        "    \n",
        "    net_accuracy = 100 * (tp + tn)/tot_count\n",
        "    \n",
        "    for i in range(n):\n",
        "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
        "        class_accuracy[i] =  float(class_accuracy[i].cpu().numpy())\n",
        "\n",
        "    prec = tp / (tp + fp)\n",
        "    rec = tp / (tp + fn)\n",
        "    dice = 2 * prec * rec / (prec + rec)\n",
        "    prec_nc = tn / (tn + fn)\n",
        "    rec_nc = tn / (tn + fp)\n",
        "    \n",
        "    pr_rec = [prec, rec, dice, prec_nc, rec_nc]\n",
        "    \n",
        "    k = kappa(tp, tn, fp, fn)\n",
        "    \n",
        "    return {'net_loss': net_loss, \n",
        "            'net_accuracy': net_accuracy, \n",
        "            'class_accuracy': class_accuracy, \n",
        "            'precision': prec, \n",
        "            'recall': rec, \n",
        "            'dice': dice, \n",
        "            'kappa': k}\n",
        "\n",
        "results = test(test_dataset)\n",
        "pprint(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ifeLG8otCD8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqSuwfg2tCED"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZvn79GEtCER"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHJRkQnltCEZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRYCeSd9tCEg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}